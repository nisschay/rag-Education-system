import PyPDF2
from typing import List, Dict
import re
from app.services.gemini_service import gemini_service

class DocumentProcessor:
    def __init__(self):
        self.chunk_size = 800  # tokens approximately
        self.overlap = 100
    
    async def extract_text_from_pdf(self, pdf_path: str) -> str:
        """Extract text from PDF"""
        text = ""
        with open(pdf_path, 'rb') as file:
            pdf_reader = PyPDF2.PdfReader(file)
            for page_num, page in enumerate(pdf_reader.pages):
                page_text = page.extract_text()
                text += f"\n--- Page {page_num + 1} ---\n{page_text}"
        return text
    
    def split_by_units(self, text: str, unit_structure: List[Dict]) -> Dict[str, str]:
        """
        Split document by units based on user-provided structure
        unit_structure: [{"name": "Unit 1: Intro", "keywords": ["introduction", "chapter 1"]}]
        """
        unit_texts = {}
        
        # Simple keyword-based splitting (can be improved)
        for unit in unit_structure:
            pattern = '|'.join(unit.get('keywords', []))
            # Find sections matching keywords
            # This is simplified - in production, use better heuristics
            matches = re.finditer(f'({pattern})', text, re.IGNORECASE)
            # Extract text around matches
            # TODO: Implement proper section extraction
            unit_texts[unit['name']] = text  # Placeholder
        
        return unit_texts
    
    async def create_hierarchical_chunks(
        self, 
        text: str, 
        unit_id: int,
        unit_name: str
    ) -> List[Dict]:
        """Create hierarchical chunks from text"""
        chunks = []
        
        # Level 4: Raw chunks (detailed content)
        raw_chunks = self._chunk_text(text, self.chunk_size, self.overlap)
        for i, chunk in enumerate(raw_chunks):
            chunks.append({
                "content": chunk,
                "chunk_type": "raw",
                "metadata": {
                    "unit_id": unit_id,
                    "unit_name": unit_name,
                    "hierarchy_level": 4,
                    "chunk_index": i,
                    "total_chunks": len(raw_chunks)
                }
            })
        
        # Level 3: Section-level chunks (larger semantic chunks)
        sections = self._chunk_text(text, self.chunk_size * 2, self.overlap)
        for i, section in enumerate(sections):
            chunks.append({
                "content": section,
                "chunk_type": "detailed",
                "metadata": {
                    "unit_id": unit_id,
                    "unit_name": unit_name,
                    "hierarchy_level": 3,
                    "chunk_index": i
                }
            })
        
        # Level 2: Unit summary (generated by Gemini)
        unit_summary = await self._generate_summary(text, "unit")
        chunks.append({
            "content": unit_summary,
            "chunk_type": "summary_l2",
            "metadata": {
                "unit_id": unit_id,
                "unit_name": unit_name,
                "hierarchy_level": 2
            }
        })
        
        return chunks
    
    def _chunk_text(self, text: str, chunk_size: int, overlap: int) -> List[str]:
        """Split text into overlapping chunks"""
        words = text.split()
        chunks = []
        
        i = 0
        while i < len(words):
            chunk = ' '.join(words[i:i + chunk_size])
            chunks.append(chunk)
            i += chunk_size - overlap
        
        return chunks
    
    async def _generate_summary(self, text: str, level: str) -> str:
        """Generate summary using Gemini"""
        prompt = f"""Summarize the following text for a {level}-level overview.
Be comprehensive but concise (3-5 paragraphs).

Text:
{text[:8000]}  # Limit input size

Summary:"""
        
        summary = await gemini_service.generate_text(prompt)
        return summary

document_processor = DocumentProcessor()
